{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FITExplainer:\n",
    "    def __init__(self, model, generator=None,activation=torch.nn.Softmax(-1),n_classes=2):\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.generator = generator\n",
    "        self.base_model = model.to(self.device)\n",
    "        self.activation = activation\n",
    "        self.n_classes=n_classes\n",
    "\n",
    "    def fit_generator(self, generator_model, train_loader, test_loader, n_epochs=300,cv=0):\n",
    "        train_joint_feature_generator(generator_model, train_loader, test_loader, generator_type='joint_generator',\n",
    "                                      n_epochs=300, lr=0.001, weight_decay=0,cv=cv)\n",
    "        self.generator = generator_model.to(self.device)\n",
    "\n",
    "    def attribute(self, x, y, n_samples=10, retrospective=False, distance_metric='kl',subsets=None):\n",
    "        \"\"\"\n",
    "        Compute importance score for a sample x, over time and features\n",
    "        :param x: Sample instance to evaluate score for. Shape:[batch, features, time]\n",
    "        :param n_samples: number of Monte-Carlo samples\n",
    "        :return: Importance score matrix of shape:[batch, features, time]\n",
    "        \"\"\"\n",
    "        self.generator.eval()\n",
    "        self.generator.to(self.device)\n",
    "        x = x.to(self.device)\n",
    "        _, n_features, t_len = x.shape\n",
    "        score = np.zeros(list(x.shape))\n",
    "        if retrospective:\n",
    "            p_y_t = self.activation(self.base_model(x))\n",
    "\n",
    "        for t in range(1, t_len):\n",
    "            if not retrospective:\n",
    "                p_y_t = self.activation(self.base_model(x[:, :, :t+1]))\n",
    "                p_tm1 = self.activation(self.base_model(x[:,:,0:t]))\n",
    "\n",
    "            for i in range(n_features):\n",
    "                x_hat = x[:,:,0:t+1].clone()\n",
    "                div_all=[]\n",
    "                t1_all=[]\n",
    "                t2_all=[]\n",
    "                for _ in range(n_samples):\n",
    "                    x_hat_t, _ = self.generator.forward_conditional(x[:, :, :t], x[:, :, t], [i])\n",
    "                    x_hat[:, :, t] = x_hat_t\n",
    "                    y_hat_t = self.activation(self.base_model(x_hat))\n",
    "                    if distance_metric == 'kl':\n",
    "                        if type(self.activation).__name__==type(torch.nn.Softmax(-1)).__name__:\n",
    "                            div = torch.sum(torch.nn.KLDivLoss(reduction='none')(torch.log(p_tm1), p_y_t), -1) - \\\n",
    "                             torch.sum(torch.nn.KLDivLoss(reduction='none')(torch.log(y_hat_t), p_y_t), -1)\n",
    "                            lhs = torch.sum(torch.nn.KLDivLoss(reduction='none')(torch.log(p_tm1), p_y_t), -1)\n",
    "                            rhs = torch.sum(torch.nn.KLDivLoss(reduction='none')(torch.log(y_hat_t), p_y_t), -1)\n",
    "                            # div = torch.where(rhs>lhs, torch.zeros(rhs.shape), rhs/lhs)\n",
    "                        else:\n",
    "                            t1 = kl_multilabel(p_y_t, p_tm1)\n",
    "                            t2 = kl_multilabel(p_y_t, y_hat_t)\n",
    "                            div,_ = torch.max(t1 - t2,dim=1)\n",
    "                            #div = div[:,0] #flatten\n",
    "                        div_all.append(div.cpu().detach().numpy())\n",
    "                    elif distance_metric == 'mean_divergence':\n",
    "                        div = torch.abs(y_hat_t - p_y_t)\n",
    "                        div_all.append(np.mean(div.detach().cpu().numpy(), -1))\n",
    "                    elif distance_metric=='LHS':\n",
    "                        div = torch.sum(torch.nn.KLDivLoss(reduction='none')(torch.log(p_tm1), p_y_t), -1)\n",
    "                        div_all.append(div.cpu().detach().numpy())\n",
    "                    elif distance_metric=='RHS':\n",
    "                        div = torch.sum(torch.nn.KLDivLoss(reduction='none')(torch.log(y_hat_t), p_y_t), -1)\n",
    "                        div_all.append(div.cpu().detach().numpy())\n",
    "                E_div = np.mean(np.array(div_all),axis=0)\n",
    "                if distance_metric =='kl':\n",
    "                    # score[:, i, t] = E_div\n",
    "                    score[:, i, t] = 2./(1+np.exp(-5*E_div)) - 1\n",
    "                elif distance_metric=='mean_divergence':\n",
    "                    score[:, i, t] = 1-E_div\n",
    "                else:\n",
    "                    score[:, i, t] = E_div\n",
    "        return score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
